# ğŸš— NAVIS â€” Vision-Language Multimodal Dataset Navigator  
### Built with **Latitude AI (Ford)** as part of the **Break Through Tech AI Studio Program (Fall 2025)**

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue" />
  <img src="https://img.shields.io/badge/FastAPI-Backend-green" />
  <img src="https://img.shields.io/badge/Next.js-Frontend-black" />
  <img src="https://img.shields.io/badge/CLIP-VLM-orange" />
  <img src="https://img.shields.io/badge/BLIP-Captioning-purple" />
  <img src="https://img.shields.io/badge/FAISS-VectorSearch-yellow" />
  <img src="https://img.shields.io/badge/Postgres-Supabase-blue" />
  <img src="https://img.shields.io/badge/Latitude_AI-Ford-red" />
  <img src="https://img.shields.io/badge/Docker-Ready-lightgrey" />
</p>

---

# ğŸŒŸ Project Highlights
- Built an end-to-end **semantic visual search engine** for autonomous-driving datasets  
- Reduced dataset exploration time from **4+ hours** to **seconds**  
- Indexed **~25,000 frames** with CLIP embeddings + FAISS HNSW search  
- BLIP captioning automatically summarizes retrieved scenes  
- Fully modular system enabling **plug-and-play extension** to new datasets  

---

# ğŸ‘¥ Team Members

| Name | GitHub | Contribution Area |
|------|--------|-------------------|
| **Husnain Khaliq** | https://github.com/huscse | UI / Backend - Model Integration|
| **Gagan Charagondla** | https://github.com/gagan12334 | VLM / Backend |
| **Keerthana Venkatesan** | https://github.com/keerthanavenkatesan415 | VLM / Data Processing|
| **Lissette Solano** | https://github.com/Lissette31 | Data Processing
| **Manasvi** | https://github.com/ManuPer4 | Data Processing|
| **Yesun Ang** | https://github.com/yesun-ang | VLM / Data Processing |
| **Erica Li** | https://github.com/erica-1i | UI/UX- Frontend|

---

# ğŸ¯ Project Overview

Autonomous-vehicle (AV) companies such as **Latitude AI (Ford)** collect massive driving datasets.  
Engineers often spend **multiple hours** searching for rare but safety-critical casesâ€”for example:  

> "a pedestrian crossing at night in the rain"

NAVIS automates and accelerates this process with:

- **Natural-language search** (â€œcar turning left at nightâ€)  
- **Cross-dataset semantic matching** using CLIP embeddings  
- **Fast vector retrieval** via FAISS  
- **BLIP scene captioning** for quick understanding  
- **An interactive web interface** for exploration  

### Business Relevance
NAVIS helps Latitude AI:
- Quickly surface **long-tail safety events**  
- Reduce manual review workloads  
- Improve scenario mining for **edge-case validation**  
- Accelerate AV testing and deployment readiness  

---

# ğŸ§© Dataset Exploration (EDA)

NAVIS works with **three real-world autonomous-driving datasets**, using scalable preprocessing and embedding pipelines.

## ğŸ“¦ Datasets Used
| Dataset | Frames Used | Description |
|--------|-------------|-------------|
| **BDD100K** | ~10,000 | Diverse urban scenes, varied weather/time-of-day |
| **KITTI Raw** | 2â€“3 sequences (~6â€“9k frames) | High-quality calibrated driving data |
| **Argoverse 1.1** | 2â€“3 logs (~6â€“10k frames) | Continuous city driving sequences |

Total processed frames: **~25,000**

---

## ğŸ—‚ Example Metadata Schema

```json
{
  "frame_id": "argo_001234",
  "dataset": "Argoverse",
  "filepath": "argo/log03/001234.jpg",
  "timestamp": 1624029342,
  "weather": "rainy",
  "timeofday": "night",
  "sequence": "log03",
  "gps": [40.44062, -79.99593]
}
```

ğŸ›  **Data Preprocessing**

### 1. Frame Extraction
- Extracted frames at 10â€“20 FPS  
- Converted all images to RGB  
- Resized to **640Ã—384**

### 2. Metadata Cleaning
- Normalized weather/time labels (BDD100K)  
- Unified timestamp formatting  
- Standardized camera fields across datasets  

### 3. CLIP Embedding Generation
- Model: `openai/clip-vit-base-patch32`  
- Extracted **512-D embeddings per frame**  
- Saved outputs to:  
  - `data/embeddings/*.npy`  
  - `data/metadata/*.jsonl`

### 4. FAISS Index Construction
- Index type: **HNSW**  
- Hyperparameters:  
  - `M = 32`  
  - `efConstruction = 50`  
- Achieved **<0.5 sec** per query

### 5. BLIP Caption Generation
- Generated short scene descriptions  
- Implemented **caption caching** to avoid recomputation  

---

# ğŸ“Š EDA Insights

## 1. Dataset Distribution

```mermaid
pie
  title Frame Distribution by Dataset
  "BDD100K (10k)" : 10000
  "KITTI (7k)" : 7000
  "Argoverse (8k)" : 8000
```

## 2. Time-of-Day in BDD100K
```
Day:         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (61%)
Night:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              (27%)
Dusk/Dawn:   â–ˆâ–ˆâ–ˆâ–ˆ                  (12%)
```

## 3. Weather Diversity (BDD100K Sample)
```
Clear:       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (70%)
Rainy:       â–ˆâ–ˆâ–ˆâ–ˆ                (15%)
Overcast:    â–ˆâ–ˆâ–ˆ                 (10%)
Snow:        â–ˆ                   (5%)
```

---

# ğŸ¤– Model Development & Method Justification

### Why CLIP?
- State-of-the-art visionâ€“language alignment  
- Excels in cross-dataset generalization  
- Strong performance for semantic retrieval  

### Why BLIP?
- Produces clean, human-understandable captions  
- Helps users interpret matched frames  

### Why FAISS?
- Industry-standard ANN search  
- High recall + low latency  
- Scales to millions of vectors  

---

# ğŸ”§ Technical Architecture

## ASCII System Diagram
```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      Next.js Frontend      â”‚
         â”‚  - Search bar              â”‚
         â”‚  - Image grid              â”‚
         â”‚  - Summaries               â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚  REST / JSON
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚       FastAPI Backend      â”‚
         â”‚  - Encode text query       â”‚
         â”‚  - Query FAISS index       â”‚
         â”‚  - Retrieve metadata       â”‚
         â”‚  - BLIP captioning         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚               FAISS Index (ANN)            â”‚
 â”‚   512D CLIP embeddings for all frames      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚      Raw Datasets (BDD100K, KITTI, Argo)   â”‚
 â”‚   + metadata + preprocessing pipelines      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Mermaid Architecture Diagram

```mermaid
flowchart TD

A[User Query] --> B[Next.js Frontend UI]
B --> C[FastAPI Backend]

C --> D[CLIP Text Encoder]
C --> E[FAISS Vector Search]
E --> F[Embeddings + Metadata]

F --> G[Dataset Frames]

C --> H[BLIP Caption Generator]
H --> C
C --> B
```

---

# ğŸ“ Repository Structure

```
vlm-dataset-navigator/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ main.py                      # FastAPI application entry point
â”‚   â”‚
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search.py                    # Semantic search with dataset diversity
â”‚   â”‚   â”œâ”€â”€ caption.py                   # BLIP-large caption generation
â”‚   â”‚   â”œâ”€â”€ media.py                     # Google Drive media serving
â”‚   â”‚   â”œâ”€â”€ frames.py                    # Frame metadata endpoints
â”‚   â”‚   â”œâ”€â”€ datasets.py                  # Dataset information API
â”‚   â”‚   â””â”€â”€ sequences.py                 # Sequence browsing endpoints
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_embed.py                # CLIP text embedding service
â”‚   â”‚   â”œâ”€â”€ drive.py                     # Google Drive API integration
â”‚   â”‚   â””â”€â”€ faiss_search.py              # FAISS similarity search wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ postgres.py                  # PostgreSQL connection pool
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ init_db.py                   # Database schema initialization
â”‚   â”‚   â”œâ”€â”€ ingest_kitti.py              # KITTI dataset ingestion
â”‚   â”‚   â”œâ”€â”€ ingest_bdd10k_gdrive_auto.py # BDD100K automated ingestion
â”‚   â”‚   â”œâ”€â”€ embed_kitti.py               # Generate CLIP embeddings for KITTI
â”‚   â”‚   â”œâ”€â”€ embed_bdd10k.py              # Generate CLIP embeddings for BDD100K
â”‚   â”‚   â”œâ”€â”€ detect_objects.py            # YOLOv8 batch object detection
â”‚   â”‚   â”œâ”€â”€ build_faiss_index.py         # Build FAISS vector index
â”‚   â”‚   â””â”€â”€ caption_frames.py            # BLIP-large batch captioning
â”‚   â”‚
â”‚   â”œâ”€â”€ faiss_indexes/
â”‚   â”‚   â”œâ”€â”€ combined.index               # FAISS IndexFlatIP (2,794 vectors)
â”‚   â”‚   â”œâ”€â”€ combined_mapping.npy         # Frame ID to FAISS index mapping
â”‚   â”‚   â”œâ”€â”€ kitti.index                  # KITTI-only index (legacy)
â”‚   â”‚   â””â”€â”€ kitti_mapping.npy            # KITTI mapping (legacy)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py                  # Environment configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                            # Sample data (gitignored)
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”‚
â”‚   â”œâ”€â”€ secrets/                         # API credentials (gitignored)
â”‚   â”‚   â”œâ”€â”€ google_drive_credentials.json
â”‚   â”‚   â””â”€â”€ .gitkeep
â”‚   â”‚
â”‚   â”œâ”€â”€ workers/                         # Background job processors
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ embedder.py                  # Helper embedding file
â”‚   â”‚
â”‚   â”œâ”€â”€ docker-compose.yml               # PostgreSQL + Adminer setup
â”‚   â”œâ”€â”€ Dockerfile                       # Backend container
â”‚   â”œâ”€â”€ requirements.txt                 # Python dependencies
â”‚   â”œâ”€â”€ .dockerignore                    # Docker build exclusions
â”‚   â”œâ”€â”€ .gitignore                       # Git exclusions
â”‚   â”œâ”€â”€ navis_backup.sql                 # Database backup
â”‚   â””â”€â”€ README.md                        # Backend documentation
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ About.jsx                # Project overview page
â”‚   â”‚   â”‚   â”œâ”€â”€ CardStack.jsx            # Tech stack information cards
â”‚   â”‚   â”‚   â”œâ”€â”€ Demo.jsx                 # Demo video display
â”‚   â”‚   â”‚   â”œâ”€â”€ DynamicGreeting.jsx      # Personalized greeting component
â”‚   â”‚   â”‚   â”œâ”€â”€ Footer.jsx               # Footer component
â”‚   â”‚   â”‚   â”œâ”€â”€ GithubButton.jsx         # GitHub repository link button
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.jsx               # Page header component
â”‚   â”‚   â”‚   â”œâ”€â”€ Hero.jsx                 # Landing page hero section
â”‚   â”‚   â”‚   â”œâ”€â”€ HeroSection.jsx          # Hero section with animations
â”‚   â”‚   â”‚   â”œâ”€â”€ LandingPage.jsx          # Complete landing page
â”‚   â”‚   â”‚   â”œâ”€â”€ Navigation.jsx           # Main navigation with auth
â”‚   â”‚   â”‚   â”œâ”€â”€ ProfileModal.jsx         # User profile modal
â”‚   â”‚   â”‚   â”œâ”€â”€ ResultCard.jsx           # Individual frame card with metadata
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchBar.jsx            # Natural language search input
â”‚   â”‚   â”‚   â”œâ”€â”€ SearchResults.jsx        # Results grid with diversity display
â”‚   â”‚   â”‚   â”œâ”€â”€ SignIn.jsx               # Sign in form component
â”‚   â”‚   â”‚   â”œâ”€â”€ SignUp.jsx               # Sign up form component
â”‚   â”‚   â”‚   â”œâ”€â”€ Team.jsx                 # Team member search animation
â”‚   â”‚   â”‚   â””â”€â”€ TopNavBarHeader.jsx      # Top navigation bar
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ search/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # Main search interface
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ land/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # Landing page route
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ about/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # About page route
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ demo/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # Demo page route
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ bookmarks/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # Saved frames page
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ signin/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # Authentication page
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ signup/
â”‚   â”‚   â”‚   â””â”€â”€ page.jsx                 # User registration
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ layout.js                    # Root layout with providers
â”‚   â”‚   â”œâ”€â”€ globals.css                  # Global styles + Tailwind
â”‚   â”‚   â””â”€â”€ page.js                      # Homepage (redirects to /land)
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ useAuthSession.js            # Supabase client configuration
â”‚   â”‚   â”œâ”€â”€ supabaseClient.js            # Supabase client configuration
â”‚   â”‚   â””â”€â”€ api.js                       # Backend API wrapper functions
â”‚   â”‚   â””â”€â”€ bookmarks.js                 # Backend API wrapper functions
â”‚   â”‚
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ images/                      # Static images
â”‚   â”‚   â”œâ”€â”€ Navis-demo.mp4               # Demo video (not self-hosted)
â”‚   â”‚   â””â”€â”€ favicon.ico                  # Site icon
â”‚   â”‚
â”‚   â”œâ”€â”€ .env.local                       # Environment variables (gitignored)
â”‚   â”œâ”€â”€ .gitignore                       # Git exclusions
â”‚   â”œâ”€â”€ next.config.js                   # Next.js configuration
â”‚   â”œâ”€â”€ tailwind.config.js               # Tailwind CSS configuration
â”‚   â”œâ”€â”€ postcss.config.js                # PostCSS configuration
â”‚   â”œâ”€â”€ package.json                     # npm dependencies
â”‚   â”œâ”€â”€ package-lock.json                # Dependency lock file
â”‚   â””â”€â”€ README.md                        # Frontend documentation
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ frontend-deploy.yml          # Vercel deployment workflow
â”‚
â”œâ”€â”€ .gitignore                           # Root-level git exclusions
â””â”€â”€ README.md                            # Main project documentation
```

---

# ğŸ—ï¸ Code Highlights

### Key Backend Files

- `backend/search/query.py`  
  Main FAISS retrieval logic  

- `backend/models/clip_model.py`  
  Loads CLIP and encodes frames  

- `backend/models/blip_model.py`  
  Caption generator  

- `backend/api/routes.py`  
  Request/response handling  

### Key Frontend Files

- `frontend/components/ResultCard.tsx`  
  Displays results + captions  

- `frontend/pages/index.tsx`  
  Search UI  

---

# ğŸ“ˆ Results & Key Findings
### Sample Query Results

| Query | Example Retrieved Result |
|--------|---------------------------|
| â€œcar turning left at nightâ€ | Night scene with sedan turning left |
| â€œbicycle crossing intersectionâ€ | Cyclist in BDD100K intersection |
| â€œrainy road pedestriansâ€ | KITTI pedestrian frame under rain |

---

# ğŸ§  Discussion & Reflection

### What Worked
- CLIP produced strong cross-dataset similarity  
- FAISS provided fast and reliable retrieval  
- Next.js + FastAPI supported modular development  

### What Didnâ€™t
- BLIP struggles in low-light conditions  
- Embedding generation slow without GPU  
- Metadata unification required large manual effort  

### What We Learned
- Metadata standardization is critical  
- CLIP is robust across heterogeneous datasets  
- Good UI design dramatically improves user experience  

---

# ğŸ”® Next Steps

- Expand to full 100k-frame BDD100K dataset  
- Add temporal/video-level semantic search  
- Integrate larger CLIP models (ViT-L/14)  
- Cluster frames to auto-generate scenario tags  
- Deploy a fully hosted demo  

---

# ğŸ›  Installation & Running

### Option A â€” Docker (Recommended)

```bash
git clone https://github.com/<your-org>/vlm-dataset-navigator
cd vlm-dataset-navigator
docker-compose up --build
```

Frontend â†’ http://localhost:3000  
Backend â†’ http://localhost:8000  

---

### Option B â€” Manual Setup

#### Backend

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cd backend
uvicorn main:app --reload
```

#### Frontend

```bash
cd frontend
npm install
npm run dev
```

---

# â• Adding New Datasets

1. Add dataset to `/data/`
2. Run embedding notebook:  
   `notebooks/generate_embeddings.ipynb`
3. Build FAISS index:  

```bash
python backend/search/build_faiss_index.py
```

4. Restart backend  

---

# ğŸ§ª Testing

```bash
pytest tests/
```

---

# â— Troubleshooting

### Backend unreachable
- Confirm backend running on localhost:8000  
- Update CORS in `main.py`  

### FAISS fails to load
```bash
python backend/search/build_faiss_index.py
```

### BLIP out-of-memory (Colab)
- Lower batch size  

### Docker memory issues
- Allocate **8GB+ RAM**  

---

# ğŸ“œ License
This project is open source

---

# ğŸ™ Acknowledgements

Built as part of **Break Through Tech AI Studio (Fall 2025)** with support from:

- **Latitude AI (Ford)** â€“ Host organization  
- **Sai Duddu** â€“ AI Studio Coach  
- **Shivam Gautam & Nilesh Choubey (CMU)** â€“ Challenge Advisors  
